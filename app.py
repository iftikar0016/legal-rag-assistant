import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
from rag_index_builder import build_index_from_pdf
from tools import retrieve_legal_context
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core.tools import FunctionTool

# Load environment variables (for local development)
load_dotenv()

# Support both local .env and Streamlit Cloud secrets
def get_secret(key: str, default: str = None) -> str:
    """Get secret from Streamlit secrets or environment variables."""
    try:
        return st.secrets[key]
    except (KeyError, FileNotFoundError):
        return os.getenv(key, default)

OPENAI_API_KEY = get_secret("OPENAI_API_KEY")
OPENAI_BASE_URL = get_secret("OPENAI_BASE_URL")

# Core chat function
async def run_agent(query):
    """Run the legal assistant agent to answer queries."""
    
    # Define the tool
    retrieve_tool = FunctionTool(
        retrieve_legal_context,
        description="Retrieve relevant legal context from the indexed legal documents based on the query."
    )
    
    # Create the assistant agent
    legal_assistant = AssistantAgent(
        name="LegalAssistant",
        model_client={
            "config_list": [{
                "model": "gpt-5-mini",
                "api_key": OPENAI_API_KEY,
                "base_url": OPENAI_BASE_URL,
            }],
            "temperature": 0.7,
        },
        tools=[retrieve_tool],
        system_message="You are a helpful legal assistant that answers user queries by using the 'retrieve_legal_context' tool to find relevant information from legal documents."
    )
    
    # Create termination condition
    termination = TextMentionTermination("TERMINATE")
    
    # Create team
    team = RoundRobinGroupChat([legal_assistant], termination_condition=termination)
    
    # Run the chat
    result = await team.run(task=query)
    
    # Extract the final answer
    if result.messages:
        for msg in reversed(result.messages):
            if hasattr(msg, 'content') and msg.content and "TERMINATE" not in str(msg.content):
                return str(msg.content), result.messages
    
    return "No valid answer was generated by the assistant.", result.messages if hasattr(result, 'messages') else []

# Streamlit app layout
st.set_page_config(page_title="ğŸ“„ Legal RAG Assistant", layout="wide")
st.markdown("""
    <div style="
        background: linear-gradient(135deg, #e0f7fa, #ffdde1);
        padding: 25px 35px;
        border-radius: 15px;
        max-width: 850px;
        margin: 0 auto 30px auto;
        text-align: center;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
    ">
        <h1 style="
            color: #1a237e;
            font-size: 2.8em;
            margin-bottom: 12px;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        ">
            âš–ï¸ Legal RAG Assistant
        </h1>
        <p style="
            color: #37474f;
            font-size: 1.15em;
            font-weight: 500;
            line-height: 1.6;
        ">
            Ask your legal questions with confidence ğŸ’¼<br/>
            Powered by Retrieval-Augmented Generation (RAG) ğŸ§ ğŸ”
        </p>
    </div>
""", unsafe_allow_html=True)



st.markdown("Upload your **legal documents** and ask your legal questions. Our AI assistant will respond with contextual answers extracted directly from your document. ğŸ”ğŸ“˜")

# File upload
uploaded_file = st.file_uploader("ğŸ“ Upload a legal PDF document", type=["pdf"])

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_pdf_path = tmp_file.name

    st.success("âœ… PDF uploaded successfully!")

    with st.spinner("ğŸ”§ Building FAISS index from uploaded PDF..."):
        build_index_from_pdf(tmp_pdf_path, persist_dir="rag_faiss_store")
    st.success("ğŸ“š Index built and ready to go!")

    # Query section
    st.markdown("---")
    st.subheader("ğŸ’¬ Ask Your Legal Question")
    query = st.text_input("Type your legal query here:")

    if query:
        with st.spinner("ğŸ¤– Retrieving answer from Legal Assistant..."):
            import asyncio
            answer, chat_history = asyncio.run(run_agent(query))

        st.markdown("### ğŸ§  Agent's Answer:")
        st.success(answer)

        # Optional: Expandable chat history
        with st.expander("ğŸ“œ View Full Chat History"):
            for msg in chat_history:
                content = str(msg.content) if hasattr(msg, 'content') else str(msg)
                if content and "TERMINATE" not in content:
                    st.markdown(f"**Message**: {content}")
else:
    st.info("ğŸ“‚ Please upload a legal PDF to begin.")