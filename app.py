import streamlit as st
import os
import tempfile
from dotenv import load_dotenv
from rag_index_builder import build_index_from_pdf
from tools import retrieve_legal_context
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent

# Load environment variables (for local development)
load_dotenv()

# Support both local .env and Streamlit Cloud secrets
def get_secret(key: str, default: str = None) -> str:
    """Get secret from Streamlit secrets or environment variables."""
    try:
        return st.secrets[key]
    except (KeyError, FileNotFoundError):
        return os.getenv(key, default)

OPENAI_API_KEY = get_secret("OPENAI_API_KEY")
OPENAI_BASE_URL = get_secret("OPENAI_BASE_URL")

# LLM config
llm_config = {
    "config_list": [
        {
            "api_key": OPENAI_API_KEY,
            "base_url": OPENAI_BASE_URL,
            "model": "gpt-5-mini",  # Set to your preferred OpenAI model
        }
    ],
    "temperature": 1
}

# Define termination check
def is_termination_msg(msg):
    return msg.get("content") and "TERMINATE" in msg["content"]

# Core chat function
def run_agent(query):
    legal_assistant = AssistantAgent(
        name="LegalAssistant",
        system_message=(
            "You are a helpful legal assistant that answers user queries ONLY by calling the 'retrieve_legal_context' tool. "
            "After answering, always respond with 'TERMINATE' to end the chat."
        ),
        llm_config=llm_config,
    )

    user = UserProxyAgent(
        name="User",
        llm_config=False,
        human_input_mode="NEVER",
        is_termination_msg=is_termination_msg,
        code_execution_config={"use_docker": False}
    )

    legal_assistant.register_for_llm(
        name="retrieve_legal_context",
        description="Retrieve relevant legal context from the indexed legal documents based on the query."
    )(retrieve_legal_context)

    user.register_for_execution(
        name="retrieve_legal_context"
    )(retrieve_legal_context)

    chat_result = user.initiate_chat(
        legal_assistant,
        message=query,
        summary_method="reflection_with_llm"
    )

    history = getattr(chat_result, "chat_history", None)
    if history is None:
        return "No chat history found.", []

    # Extract final message
    for msg in reversed(history):
        if msg.get("role") == "user" and msg.get("name") == "LegalAssistant":
            final_content = msg.get("content", "").replace("TERMINATE", "").strip()
            return final_content, history

    return "No valid answer was generated by the assistant.", history

# Streamlit app layout
st.set_page_config(page_title="ğŸ“„ Legal RAG Assistant", layout="wide")
st.markdown("""
    <div style="
        background: linear-gradient(135deg, #e0f7fa, #ffdde1);
        padding: 25px 35px;
        border-radius: 15px;
        max-width: 850px;
        margin: 0 auto 30px auto;
        text-align: center;
        box-shadow: 0 8px 25px rgba(0, 0, 0, 0.1);
    ">
        <h1 style="
            color: #1a237e;
            font-size: 2.8em;
            margin-bottom: 12px;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        ">
            âš–ï¸ Legal RAG Assistant
        </h1>
        <p style="
            color: #37474f;
            font-size: 1.15em;
            font-weight: 500;
            line-height: 1.6;
        ">
            Ask your legal questions with confidence ğŸ’¼<br/>
            Powered by Retrieval-Augmented Generation (RAG) ğŸ§ ğŸ”
        </p>
    </div>
""", unsafe_allow_html=True)



st.markdown("Upload your **legal documents** and ask your legal questions. Our AI assistant will respond with contextual answers extracted directly from your document. ğŸ”ğŸ“˜")

# File upload
uploaded_file = st.file_uploader("ğŸ“ Upload a legal PDF document", type=["pdf"])

if uploaded_file is not None:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_pdf_path = tmp_file.name

    st.success("âœ… PDF uploaded successfully!")

    with st.spinner("ğŸ”§ Building FAISS index from uploaded PDF..."):
        build_index_from_pdf(tmp_pdf_path, persist_dir="rag_faiss_store")
    st.success("ğŸ“š Index built and ready to go!")

    # Query section
    st.markdown("---")
    st.subheader("ğŸ’¬ Ask Your Legal Question")
    query = st.text_input("Type your legal query here:")

    if query:
        with st.spinner("ğŸ¤– Retrieving answer from Legal Assistant..."):
            answer, chat_history = run_agent(query)

        st.markdown("### ğŸ§  Agent's Answer:")
        st.success(answer)

        # Optional: Expandable chat history
        with st.expander("ğŸ“œ View Full Chat History"):
            for msg in chat_history:
                role = msg.get("role", "").capitalize()
                name = msg.get("name", "")
                content = msg.get("content", "")
                if content:
                    st.markdown(f"**{role} ({name})**: {content}")
else:
    st.info("ğŸ“‚ Please upload a legal PDF to begin.")